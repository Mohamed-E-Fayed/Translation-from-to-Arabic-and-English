\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{2}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Neural Machine Translation (NMT)}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:nmt}{{2}{3}{Neural Machine Translation (NMT)}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Background}{3}{section.2.1}\protected@file@percent }
\newlabel{sec:nmt-background}{{2.1}{3}{Background}{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Bidirectional Encoder Representations from Transformers (BERT)}{4}{section.2.2}\protected@file@percent }
\newlabel{sec:bert}{{2.2}{4}{Bidirectional Encoder Representations from Transformers (BERT)}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Background}{4}{subsection.2.2.1}\protected@file@percent }
\newlabel{ssec:bert-background}{{2.2.1}{4}{Background}{subsection.2.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Unsupervised Feature-Based Approaches}{4}{section*.2}\protected@file@percent }
\newlabel{sssec:bert-unsupervised-feature-based-approaches}{{2.2.1}{4}{Unsupervised Feature-Based Approaches}{section*.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Unsupervised Fine-Tuning Approaches}{5}{section*.3}\protected@file@percent }
\newlabel{sssec:bert-unsupervised-fine-tuning-approaches}{{2.2.1}{5}{Unsupervised Fine-Tuning Approaches}{section*.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transfer Learning From Supervised Data}{5}{section*.4}\protected@file@percent }
\newlabel{sssec:bert-transfer-learning}{{2.2.1}{5}{Transfer Learning From Supervised Data}{section*.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special symbol added infromt of every input example, and [SEP] is a special separator token (e.g. separating questions/answers)}}{6}{figure.2.1}\protected@file@percent }
\newlabel{fig:overall-training-procedure-for-bert}{{2.1}{6}{Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architectures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special symbol added infromt of every input example, and [SEP] is a special separator token (e.g. separating questions/answers)}{figure.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}BERT Implementation}{7}{subsection.2.2.2}\protected@file@percent }
\newlabel{ssec:bert-implementation}{{2.2.2}{7}{BERT Implementation}{subsection.2.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Pre-Training BERT}{7}{section*.5}\protected@file@percent }
\newlabel{sssec:pre-training-bert}{{2.2.2}{7}{Pre-Training BERT}{section*.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings are the position embeddings.}}{8}{figure.2.2}\protected@file@percent }
\newlabel{fig:bert-input-representation}{{2.2}{8}{BERT input representation. The input embeddings are the sum of the token embeddings, the segmentation embeddings are the position embeddings}{figure.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Fine-Tuning BERT}{9}{section*.6}\protected@file@percent }
\newlabel{sssec:fine-tuning-bert}{{2.2.2}{9}{Fine-Tuning BERT}{section*.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Transformers}{10}{section.2.3}\protected@file@percent }
\newlabel{sec:transformers}{{2.3}{10}{Transformers}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Background}{10}{subsection.2.3.1}\protected@file@percent }
\newlabel{ssec:transformers-background}{{2.3.1}{10}{Background}{subsection.2.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Model Architecture}{10}{subsection.2.3.2}\protected@file@percent }
\newlabel{ssec:transformers-model-architecture}{{2.3.2}{10}{Model Architecture}{subsection.2.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Encoder And Decoder Stacks}{11}{section*.7}\protected@file@percent }
\newlabel{sssec:transformer-enc-dec-stacks}{{2.3.2}{11}{Encoder And Decoder Stacks}{section*.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Attention}{11}{section*.8}\protected@file@percent }
\newlabel{sssec:transformer-attention}{{2.3.2}{11}{Attention}{section*.8}{}}
\newlabel{eq:transformer-attention}{{2.1}{11}{Attention}{equation.2.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Transformer Model Architecture}}{12}{figure.2.3}\protected@file@percent }
\newlabel{fig:transforer-model-architecture}{{2.3}{12}{Transformer Model Architecture}{figure.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces (Left) Scaled Dot-Product Attention. (Right) Multi-Head Attention consists of several attention layers running in parallel.}}{13}{figure.2.4}\protected@file@percent }
\newlabel{fig:scaled-dot-product-attention}{{2.4}{13}{(Left) Scaled Dot-Product Attention. (Right) Multi-Head Attention consists of several attention layers running in parallel}{figure.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Position-Wise Feed-Forward Networks}{14}{section*.9}\protected@file@percent }
\newlabel{sssec:transformer-position-wise-ffn}{{2.3.2}{14}{Position-Wise Feed-Forward Networks}{section*.9}{}}
\newlabel{eq:transformer-ffn}{{2.2}{14}{Position-Wise Feed-Forward Networks}{equation.2.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Embeddings And Softmax}{15}{section*.10}\protected@file@percent }
\newlabel{sssec:transformer-embedding-and-softmax}{{2.3.2}{15}{Embeddings And Softmax}{section*.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Positional Encoding}{15}{section*.11}\protected@file@percent }
\newlabel{sssec:transformer-positional-encoding}{{2.3.2}{15}{Positional Encoding}{section*.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}BERT-fused Model}{15}{section.2.4}\protected@file@percent }
\newlabel{sec:bert-nmt}{{2.4}{15}{BERT-fused Model}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Algorithm}{16}{subsection.2.4.1}\protected@file@percent }
\newlabel{ssec:bert-nmt-algorithm}{{2.4.1}{16}{Algorithm}{subsection.2.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Model}{16}{section*.12}\protected@file@percent }
\newlabel{sssec:bert-fused-the-model}{{2.4.1}{16}{The Model}{section*.12}{}}
\newlabel{eq:bert-fused-input-for-new-layer}{{2.3}{16}{The Model}{equation.2.4.3}{}}
\newlabel{eq:bert-fused-hidden-state}{{2.4}{17}{The Model}{equation.2.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{DROP-NET TRICK}{17}{section*.13}\protected@file@percent }
\newlabel{sssec:drop-net-trick}{{2.4.1}{17}{DROP-NET TRICK}{section*.13}{}}
\newlabel{eq:bert-fused-input-for-new-layer-with-drop-net}{{2.5}{17}{DROP-NET TRICK}{equation.2.4.5}{}}
\newlabel{eq:bert-fused-hidden-state-with-drop-net}{{2.6}{17}{DROP-NET TRICK}{equation.2.4.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Discussion}{18}{section*.14}\protected@file@percent }
\newlabel{sssec:bert-fused-discussion}{{2.4.1}{18}{Discussion}{section*.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The architecture of BERT-fused model. The left and write figures represeents the BERT, encoder and decoder respectively. Dash lines denote residual connections. $H_B(red part) and H_e^L (green part)$ denote the output of the last layer from BERT and encoder.}}{19}{figure.2.5}\protected@file@percent }
\newlabel{fig:bert-fused-architecture}{{2.5}{19}{The architecture of BERT-fused model. The left and write figures represeents the BERT, encoder and decoder respectively. Dash lines denote residual connections. $H_B(red part) and H_e^L (green part)$ denote the output of the last layer from BERT and encoder}{figure.2.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Our Model}{20}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:our-model}{{3}{20}{Our Model}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}About The Data}{20}{section.3.1}\protected@file@percent }
\newlabel{sec:about-the-data}{{3.1}{20}{About The Data}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Specifications Of Model And Machine}{20}{section.3.2}\protected@file@percent }
\newlabel{sec:our-model-specs}{{3.2}{20}{Specifications Of Model And Machine}{section.3.2}{}}
\citation{citations.bib}
